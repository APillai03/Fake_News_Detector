{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dff8ae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import json\n",
    "import textstat\n",
    "import pandas as pd\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5ffd3c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "23e3fadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clickbait_keywords = [\n",
    "    \"shocking\", \"you won’t believe\", \"unbelievable\", \"exclusive\",\n",
    "    \"what happened next\", \"will blow your mind\", \"exposed\", \"top secret\",\n",
    "    \"can't miss\", \"epic\", \"amazing\", \"guaranteed\", \"crazy\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f5c17a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_metadata(text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Text length metrics\n",
    "    num_chars = len(text)\n",
    "    words = [token.text for token in doc if token.is_alpha]\n",
    "    num_words = len(words)\n",
    "    sentences = list(doc.sents)\n",
    "    num_sentences = len(sentences)\n",
    "    avg_sentence_length = round(num_words / num_sentences, 2) if num_sentences else 0\n",
    "\n",
    "    # Capitalized words\n",
    "    capitalized_words = [token.text for token in doc if token.text.isupper() and len(token.text) > 1]\n",
    "    num_caps = len(capitalized_words)\n",
    "\n",
    "    # Special punctuation\n",
    "    num_exclamations = text.count('!')\n",
    "    num_questions = text.count('?')\n",
    "\n",
    "    # Clickbait detection\n",
    "    text_lower = text.lower()\n",
    "    has_clickbait = any(word in text_lower for word in clickbait_keywords)\n",
    "\n",
    "    # Readability score\n",
    "    readability_score = textstat.flesch_reading_ease(text)\n",
    "\n",
    "    # Sentiment\n",
    "    sentiment = TextBlob(text).sentiment\n",
    "    polarity = sentiment.polarity\n",
    "    subjectivity = sentiment.subjectivity\n",
    "\n",
    "    # POS Ratios\n",
    "    total_tokens = len([token for token in doc if token.is_alpha])\n",
    "    pos_counts = {}\n",
    "    for token in doc:\n",
    "        if token.is_alpha:\n",
    "            pos = token.pos_\n",
    "            pos_counts[pos] = pos_counts.get(pos, 0) + 1\n",
    "    pos_ratios = {k: round(v / total_tokens, 3) for k, v in pos_counts.items()}\n",
    "\n",
    "    return {\n",
    "        \"num_characters\": num_chars,\n",
    "        \"num_words\": num_words,\n",
    "        \"num_sentences\": num_sentences,\n",
    "        \"avg_sentence_length\": avg_sentence_length,\n",
    "        \"num_capitalized_words\": num_caps,\n",
    "        \"num_exclamations\": num_exclamations,\n",
    "        \"num_questions\": num_questions,\n",
    "        \"has_clickbait_words\": has_clickbait,\n",
    "        \"readability_score\": readability_score,\n",
    "        \"sentiment_polarity\": polarity,\n",
    "        \"sentiment_subjectivity\": subjectivity,\n",
    "        \"pos_ratios\": pos_ratios\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7f72f3b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ind</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  ind  \\\n",
       "0             0           0    0   \n",
       "1             1           1    1   \n",
       "2             2           2    2   \n",
       "3             3           3    3   \n",
       "4             4           4    4   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date class  \n",
       "0  December 31, 2017  Fake  \n",
       "1  December 31, 2017  Fake  \n",
       "2  December 30, 2017  Fake  \n",
       "3  December 29, 2017  Fake  \n",
       "4  December 25, 2017  Fake  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f09f0352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_of_article(text):\n",
    "    words = word_tokenize(text)\n",
    "    return len(words), len(text)  # word count, character count\n",
    "\n",
    "\n",
    "def number_of_sentences(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return len(sentences)\n",
    "\n",
    "\n",
    "def average_sentence_length(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    return len(words) / len(sentences) if sentences else 0\n",
    "\n",
    "\n",
    "def number_of_capitalized_words(text):\n",
    "    words = word_tokenize(text)\n",
    "    return sum(1 for w in words if w.isupper())\n",
    "\n",
    "\n",
    "def punctuation_marks_count(text):\n",
    "    return text.count('!'), text.count('?')\n",
    "\n",
    "\n",
    "def contains_clickbait_phrases(text):\n",
    "    clickbait_keywords = [\n",
    "        \"shocking\", \"you won’t believe\", \"unbelievable\",\n",
    "        \"exclusive\", \"what happened next\", \"can't miss\",\n",
    "        \"this will blow your mind\", \"never seen before\"\n",
    "    ]\n",
    "    text_lower = text.lower()\n",
    "    return any(phrase in text_lower for phrase in clickbait_keywords)\n",
    "\n",
    "\n",
    "def flesch_kincaid_score(text):\n",
    "    try:\n",
    "        return textstat.flesch_kincaid_grade(text)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def sentiment_score_textblob(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity  # ranges from -1 to 1\n",
    "\n",
    "\n",
    "def sentiment_score_vader(text):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    score = analyzer.polarity_scores(text)\n",
    "    return score['compound']  # ranges from -1 to 1\n",
    "\n",
    "\n",
    "def pos_ratios(text):\n",
    "    words = word_tokenize(text)\n",
    "    pos_tags = pos_tag(words)\n",
    "    total = len(pos_tags)\n",
    "    if total == 0:\n",
    "        return {'adj_ratio': 0, 'noun_ratio': 0, 'propn_ratio': 0}\n",
    "    \n",
    "    adj_count = sum(1 for word, tag in pos_tags if tag.startswith('JJ'))\n",
    "    noun_count = sum(1 for word, tag in pos_tags if tag.startswith('NN'))\n",
    "    propn_count = sum(1 for word, tag in pos_tags if tag == 'NNP')\n",
    "    \n",
    "    return {\n",
    "        'adj_ratio': adj_count / total,\n",
    "        'noun_ratio': noun_count / total,\n",
    "        'propn_ratio': propn_count / total\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0a22e1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def extract_article_metadata(text):\n",
    "    word_count, char_count = length_of_article(text)\n",
    "    sentence_count = number_of_sentences(text)\n",
    "    avg_sentence_len = average_sentence_length(text)\n",
    "    capital_words = number_of_capitalized_words(text)\n",
    "    exclam_count, question_count = punctuation_marks_count(text)\n",
    "    clickbait = contains_clickbait_phrases(text)\n",
    "    readability = flesch_kincaid_score(text)\n",
    "    sentiment_blob = sentiment_score_textblob(text)\n",
    "    sentiment_vader = sentiment_score_vader(text)\n",
    "    pos_ratio = pos_ratios(text)\n",
    "\n",
    "    metadata = {\n",
    "        \"word_count\": word_count,\n",
    "        \"char_count\": char_count,\n",
    "        \"sentence_count\": sentence_count,\n",
    "        \"avg_sentence_length\": avg_sentence_len,\n",
    "        \"capitalized_word_count\": capital_words,\n",
    "        \"exclamation_count\": exclam_count,\n",
    "        \"question_count\": question_count,\n",
    "        \"has_clickbait_phrase\": clickbait,\n",
    "        \"flesch_kincaid_score\": readability,\n",
    "        \"sentiment_textblob\": sentiment_blob,\n",
    "        \"sentiment_vader\": sentiment_vader,\n",
    "        \"pos_ratios\": pos_ratio\n",
    "    }\n",
    "\n",
    "    return metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0dbd834a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    article_text = row[\"text\"]\n",
    "    metadata = extract_article_metadata(article_text)\n",
    "    metadata[\"id\"] = idx\n",
    "    metadata[\"subject\"] = row.get(\"subject\", None)\n",
    "    metadata[\"class\"] = row.get(\"class\", None)\n",
    "    metadata[\"date\"] = row.get(\"date\", None)\n",
    "\n",
    "    # Append each entry in a human-readable format\n",
    "    with open(\"article_metadata.json\", \"a\") as f:\n",
    "        json.dump(metadata, f, indent=4)\n",
    "        f.write(\",\\n\\n\")  # separate with commas and newlines for readability\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
